{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbaf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from database.connection import sql_connection\n",
    "from database.dataset import dataset_config\n",
    "import lightgbm as lgb\n",
    "from skopt.space import Real, Integer\n",
    "from modules.interface import RunModel, RunOptimization\n",
    "from plots import plots\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2b4f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM events': no such table: events",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/pandas/io/sql.py:2702\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2701\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2702\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2703\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: events",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m random_state=\u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m stdrings = \u001b[43msql_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_standard_rings_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m qrings=sql_connection.set_quarter_rings_data()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# params=[[{'cluster_eta':[0.6, 0.8]}, {'cluster_et':[20000,30000]}],\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#         [{'cluster_eta':[1.2, 1.8]}, {'cluster_et':[100000, 250000]}]]\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Definição dos bins de entrada\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/database/connection/sql_connection.py:32\u001b[39m, in \u001b[36mset_standard_rings_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_standard_rings_data\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.concat([\u001b[43mget_standard_rings_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, get_relevant_data()], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/database/connection/sql_connection.py:22\u001b[39m, in \u001b[36mget_standard_rings_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_standard_rings_data\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextrair_dados_sqlite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDoutorado\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcalibration\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcalibration_refactor\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcalibration_refactor_main\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdatabase\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mStdRings.db\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mevents\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/database/connection/sql_connection.py:13\u001b[39m, in \u001b[36mextrair_dados_sqlite\u001b[39m\u001b[34m(file_path, table_name)\u001b[39m\n\u001b[32m     10\u001b[39m sql_query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Executando a consulta SQL e carregando os resultados em um DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m dataframe = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Fechando a conexão com o banco de dados\u001b[39;00m\n\u001b[32m     16\u001b[39m conn.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/pandas/io/sql.py:497\u001b[39m, in \u001b[36mread_sql_query\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/pandas/io/sql.py:2766\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2756\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2757\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2764\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2765\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2766\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2767\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/pandas/io/sql.py:2714\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2713\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql 'SELECT * FROM events': no such table: events"
     ]
    }
   ],
   "source": [
    "random_state=1\n",
    "stdrings = sql_connection.set_standard_rings_data()\n",
    "qrings=sql_connection.set_quarter_rings_data()\n",
    "# params=[[{'cluster_eta':[0.6, 0.8]}, {'cluster_et':[20000,30000]}],\n",
    "#         [{'cluster_eta':[1.2, 1.8]}, {'cluster_et':[100000, 250000]}]]\n",
    "\n",
    "# Definição dos bins de entrada\n",
    "bins_et = {'cluster_et':[5000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 150000, 200000, 250000, 900000, 3000000]}\n",
    "bins_eta = {'cluster_eta':[0, 0.6, 0.8, 1.2, 1.37, 1.52, 1.8, 2.0, 2.2, 2.5]}\n",
    "\n",
    "# Lista para armazenar o resultado final\n",
    "params = []\n",
    "\n",
    "# Extrai as listas de valores dos dicionários\n",
    "et_values = bins_et['cluster_et']\n",
    "eta_values = bins_eta['cluster_eta']\n",
    "\n",
    "# Itera sobre cada intervalo de eta (ex: [0, 0.6], [0.6, 0.8], ...)\n",
    "for i in range(len(eta_values) - 1):\n",
    "    eta_interval = [eta_values[i], eta_values[i+1]]\n",
    "    \n",
    "    # Para cada intervalo de eta, itera sobre cada intervalo de et\n",
    "    for j in range(len(et_values) - 1):\n",
    "        et_interval = [et_values[j], et_values[j+1]]\n",
    "        \n",
    "        # Cria a estrutura de dicionário para a combinação atual\n",
    "        param_combination = [\n",
    "            {'cluster_eta': eta_interval},\n",
    "            {'cluster_et': et_interval}\n",
    "        ]\n",
    "        \n",
    "        # Adiciona a combinação à lista final\n",
    "        params.append(param_combination)\n",
    "print(len(params))\n",
    "\n",
    "dataset=dataset_config.split_dataframe(stdrings, params=params)\n",
    "stdrings_data = dataset_config.prepare_and_split_data(\n",
    "    input_type='std_rings',\n",
    "    stdrings_df=dataset,\n",
    "    train_size= 0.7,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "qrings_data= dataset_config.prepare_and_split_data(\n",
    "    input_type='quarter_rings',\n",
    "    stdrings_df=stdrings,\n",
    "    qrings_df=qrings,\n",
    "    train_size= 0.7,\n",
    "    mode='delta',\n",
    "    random_state=random_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa621649",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_lgbm = [\n",
    "    Real(name='learning_rate', low = 0.01 , high = 0.9),\n",
    "    Integer(name='num_leaves', low = 200, high = 1000),\n",
    "    Integer(name='max_depth', low = 200, high = 1000),\n",
    "    Real(name='feature_fraction', low = 0.5 , high = 1),\n",
    "    Real(name='bagging_fraction', low = 0.7 , high = 1),\n",
    "    Integer(name='bagging_freq', low = 1, high = 10),\n",
    "    Real(name='lambda_l1',low = 0.0, high = 1),\n",
    "    Real(name='lambda_l2',low = 0.5, high = 1)\n",
    "]\n",
    "\n",
    "fixed_params_lgbm = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    'objective': 'mae', \n",
    "    'metric': 'mae', \n",
    "    'num_iterations ': 500,\n",
    "    'random_state': random_state, \n",
    "    'n_jobs': -1, \n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "calibration_kwargs = {'callbacks':[lgb.early_stopping(stopping_rounds=5, verbose=False)]}\n",
    "optimization_kwargs = {'n_initial_points': 15,\n",
    "                        'n_calls': 50,\n",
    "                        'initial_point_generator': 'lhs',\n",
    "                        'random_state': random_state,\n",
    "\n",
    "                       \n",
    "                       }\n",
    "# bins_et = {'cluster_et':[5000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 150000, 200000, 250000, 900000, 3000000]}\n",
    "# bins_eta = {'cluster_eta':[0, 0.6, 0.8, 1.2, 1.37, 1.52, 1.8, 2.0, 2.2, 2.5]}\n",
    "\n",
    "parameters_run_model = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"mae\",\n",
    "    \"metric\": [\"mae\"],\n",
    "    \"num_leaves\": 30,\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"num_boost_round\": 1000,\n",
    "    \"early_stopping_rounds\": 5,\n",
    "    \"lambda_l1\": 0.0,\n",
    "    \"lambda_l2\": 0.0,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "parameters_run_model_1 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"mae\",\n",
    "    \"metric\": [\"mae\"],\n",
    "    \"num_leaves\": 139,\n",
    "    \"max_depth\": 300,\n",
    "    \"learning_rate\": 0.095,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"num_boost_round\": 1000,\n",
    "    \"early_stopping_rounds\": 5,\n",
    "    \"lambda_l1\": 0.02,\n",
    "    \"lambda_l2\": 0.72,\n",
    "    \"force_row_wise\": True,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "parameters_run_model_2 = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'mae',\n",
    "    'metric': ['mae'],\n",
    "    'num_leaves': np.int64(652),\n",
    "    'max_depth': np.int64(610),\n",
    "    'learning_rate': 0.03818654645154271,\n",
    "    'feature_fraction': 0.9039401105949553,\n",
    "    'bagging_fraction': 0.7960565139652669,\n",
    "    'bagging_freq': np.int64(6),\n",
    "    'num_boost_round': np.int64(534),\n",
    "    'early_stopping_rounds': 5,\n",
    "    'lambda_l1': 0.4069025545005863,\n",
    "    'lambda_l2': 0.6767363223435275,\n",
    "    'force_row_wise': True,\n",
    "    'verbosity': -1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_lgbm = RunOptimization()\n",
    "results=optimizer_lgbm.run(opt_class='gp_minimize',\n",
    "    model_class=\"lgbm\",\n",
    "    datasets=stdrings_data,\n",
    "    space=space_lgbm, \n",
    "    fixed_params=fixed_params_lgbm,\n",
    "    metric='mae',\n",
    "    calibration_kwargs=calibration_kwargs,\n",
    "    optimization_kwargs= optimization_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68ceeed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m merged_dataframe=[]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(optimizer_lgbm.optimizer)):\n\u001b[32m      4\u001b[39m     optimizer_lgbm.optimizer[i].fit_best_model() \n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1481\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1523\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1324\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1961\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Doutorado/calibration/calibration_refactor/venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "merged_dataframe=[]\n",
    "\n",
    "for i in range(len(optimizer_lgbm.optimizer)):\n",
    "    optimizer_lgbm.optimizer[i].fit_best_model() \n",
    "    \n",
    "    df_temp = plots.merge_dataframes(\n",
    "        stdrings.loc[optimizer_lgbm.optimizer[i].y_test.index],\n",
    "        optimizer_lgbm.optimizer[i].y_test,\n",
    "        optimizer_lgbm.optimizer[i].y_pred\n",
    "    )\n",
    "    merged_dataframe.append(df_temp)\n",
    "    print(\"Iteração\", i, \"→ shape parcial:\", df_temp.shape)\n",
    "    \n",
    "    # Calcular min e max apenas desta iteração\n",
    "    eta_min, eta_max = df_temp['cluster_eta'].min(), df_temp['cluster_eta'].max()\n",
    "    et_min, et_max   = df_temp['cluster_et'].min(), df_temp['cluster_et'].max()\n",
    "    \n",
    "    print(f\"   cluster_eta → min: {eta_min}, max: {eta_max}\")\n",
    "    print(f\"   cluster_et  → min: {et_min}, max: {et_max}\")\n",
    "\n",
    "merged_dataframes = pd.concat(merged_dataframe, ignore_index=True)\n",
    "print(merged_dataframes.columns)\n",
    "print(merged_dataframes.shape)\n",
    "# Aplicando o filtro\n",
    "df_filtrado = merged_dataframes[\n",
    "    (merged_dataframes['cluster_et'] >= 50000) & \n",
    "    (merged_dataframes['cluster_et'] <= 70000)\n",
    "]\n",
    "\n",
    "# Exibindo o shape do filtrado\n",
    "print(\"Shape do dataframe filtrado:\", df_filtrado.shape)\n",
    "\n",
    "\n",
    "bins_eta = {'cluster_eta':[0, 0.6, 0.8, 1.2, 1.37, 1.52, 1.8, 2.0, 2.2, 2.5, 3]}\n",
    "bins_et = {'cluster_et':[5000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 150000, 200000, 250000, 900000, 3000000]}\n",
    "\n",
    "et_metric=plots.parameters_filter(merged_dataframes, bins_et)\n",
    "eta_metric=plots.parameters_filter(merged_dataframes, bins_eta)\n",
    "print(len(et_metric))\n",
    "print(len(eta_metric))\n",
    "et_plot=plots.evaluate_metrics(et_metric, y_test_col='alpha', y_pred_col='y_pred')\n",
    "eta_plot=plots.evaluate_metrics(eta_metric, y_test_col='alpha', y_pred_col='y_pred')\n",
    "\n",
    "eta_plot_params={'y_data': eta_plot,}\n",
    "et_plot_params={'y_data': et_plot,}\n",
    "\n",
    "print(eta_plot_params)\n",
    "print(et_plot_params)\n",
    "\n",
    "plots.plot_errorbars(plot_configs=[eta_plot_params,],bins=bins_eta)\n",
    "plots.plot_errorbars(plot_configs=[et_plot_params, ],bins=bins_et,xscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1efc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dataframe_lgbm_models=[]\n",
    "# # PRETRAINED_MODEL_PATH = r'D:\\Doutorado\\calibration\\calibration_refactor\\debug_code\\standartRINGS.txt'\n",
    "# # loaded_booster = lgb.Booster(model_file=PRETRAINED_MODEL_PATH)\n",
    "\n",
    "# lgbm_model = RunModel()\n",
    "# results_lgbm_model=lgbm_model.run(\n",
    "#     model_class=\"lgbm\",\n",
    "#     datasets=stdrings_data,\n",
    "#     params=parameters_run_model_2,\n",
    "#     calibration_kwargs= calibration_kwargs\n",
    "# )\n",
    "# for i in range(len(lgbm_model.models)):\n",
    "#     lgbm_model.models[i].predict()\n",
    "#     #lgbm_model.models[i].predict(loaded_booster)\n",
    "#     print(type(lgbm_model.models[i].X_test))\n",
    "#     print(type(lgbm_model.models[i].y_test))\n",
    "#     if lgbm_model.models[i].X_test.index.equals(stdrings.loc[lgbm_model.models[i].y_test.index].index):\n",
    "#         print(\"Índices são iguais\")\n",
    "#     else:\n",
    "#         print(\"Índices são diferentes\")\n",
    "\n",
    "\n",
    "#     merged_dataframe_lgbm_models.append(plots.merge_dataframes(\n",
    "#         stdrings.loc[lgbm_model.models[i].y_test.index],\n",
    "#         lgbm_model.models[i].y_test, \n",
    "#         lgbm_model.models[i].y_pred)\n",
    "#     )\n",
    "# merged_dataframe_lgbm_models = pd.concat(merged_dataframe_lgbm_models, ignore_index=True)\n",
    "# print(merged_dataframe_lgbm_models.columns.to_list())\n",
    "\n",
    "# et_metric_models=plots.parameters_filter(merged_dataframe_lgbm_models, bins_et)\n",
    "# eta_metric_models=plots.parameters_filter(merged_dataframe_lgbm_models, bins_eta)\n",
    "# et_plot_models=plots.evaluate_metrics(et_metric_models, y_test_col='alpha', y_pred_col='y_pred')\n",
    "# eta_plot_models=plots.evaluate_metrics(eta_metric_models, y_test_col='alpha', y_pred_col='y_pred')\n",
    "# print(len(et_metric_models))\n",
    "# print(len(eta_metric_models))\n",
    "# et_plot_params_models={'y_data': et_plot_models,}\n",
    "# eta_plot_params_models={'y_data': eta_plot_models,}\n",
    "\n",
    "\n",
    "# print(eta_plot_params_models)\n",
    "# print(et_plot_params_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrings_merged_dataframe_lgbm_models=[]\n",
    "# # PRETRAINED_MODEL_PATH = r'D:\\Doutorado\\calibration\\calibration_refactor\\debug_code\\standartRINGS.txt'\n",
    "# # loaded_booster = lgb.Booster(model_file=PRETRAINED_MODEL_PATH)\n",
    "\n",
    "# qrings_lgbm_model = RunModel()\n",
    "# qrings_results_lgbm_model=qrings_lgbm_model.run(\n",
    "#     model_class=\"lgbm\",\n",
    "#     datasets=qrings_data,\n",
    "#     params=parameters_run_model_2,\n",
    "#     calibration_kwargs= calibration_kwargs\n",
    "# )\n",
    "# for i in range(len(qrings_lgbm_model.models)):\n",
    "#     qrings_lgbm_model.models[i].predict()\n",
    "#     #lgbm_model.models[i].predict(loaded_booster)\n",
    "#     print(type(qrings_lgbm_model.models[i].X_test))\n",
    "#     print(type(qrings_lgbm_model.models[i].y_test))\n",
    "\n",
    "#     qrings_merged_dataframe_lgbm_models.append(plots.merge_dataframes(\n",
    "#         qrings.loc[qrings_lgbm_model.models[i].y_test.index],\n",
    "#         qrings_lgbm_model.models[i].y_test, \n",
    "#         qrings_lgbm_model.models[i].y_pred)\n",
    "#     )\n",
    "# qrings_merged_dataframe_lgbm_models = pd.concat(qrings_merged_dataframe_lgbm_models, ignore_index=True)\n",
    "# print(qrings_merged_dataframe_lgbm_models.columns.to_list())\n",
    "\n",
    "# qrings_et_metric_models=plots.parameters_filter(qrings_merged_dataframe_lgbm_models, bins_et)\n",
    "# qrings_eta_metric_models=plots.parameters_filter(qrings_merged_dataframe_lgbm_models, bins_eta)\n",
    "# qrings_et_plot_models=plots.evaluate_metrics(qrings_et_metric_models, y_test_col='alpha', y_pred_col='y_pred')\n",
    "# qrings_eta_plot_models=plots.evaluate_metrics(qrings_eta_metric_models, y_test_col='alpha', y_pred_col='y_pred')\n",
    "# print(len(qrings_et_metric_models))\n",
    "# print(len(qrings_eta_metric_models))\n",
    "# qrings_et_plot_params_models={'y_data': qrings_et_plot_models,}\n",
    "# qrings_eta_plot_params_models={'y_data': qrings_eta_plot_models,}\n",
    "\n",
    "\n",
    "# print(qrings_eta_plot_params_models)\n",
    "# print(qrings_et_plot_params_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244549d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_list_of_dfs_to_excel(dfs_list, output_file, prefix=\"Sheet\"):\n",
    "#     \"\"\"\n",
    "#     Salva uma lista de DataFrames em um único arquivo Excel.\n",
    "#     Cada DataFrame vai para uma sheet diferente.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dfs_list : list[pd.DataFrame]\n",
    "#         Lista de DataFrames a serem salvos.\n",
    "#     output_file : str\n",
    "#         Caminho do arquivo .xlsx de saída.\n",
    "#     prefix : str\n",
    "#         Prefixo usado nos nomes das sheets (default: \"Sheet\").\n",
    "#     \"\"\"\n",
    "#     with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "#         for i, df in enumerate(dfs_list, start=1):\n",
    "#             sheet_name = f\"{prefix}_{i}\"\n",
    "#             df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# # Salvar as duas variáveis\n",
    "# save_list_of_dfs_to_excel(et_metric_models, \"et_metric_models.xlsx\", prefix=\"et\")\n",
    "# save_list_of_dfs_to_excel(eta_metric_models, \"eta_metric_models.xlsx\", prefix=\"eta\")\n",
    "\n",
    "\n",
    "# file_path = \"merged_dataframe_lgbm_models.xlsx\"  # nome do arquivo de saída\n",
    "\n",
    "# # Salvar DataFrame em Excel\n",
    "# merged_dataframe_lgbm_models.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
